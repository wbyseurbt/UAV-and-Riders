import argparse
import ray
from ray.tune.registry import register_env
from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv
from ray.rllib.algorithms.ppo import PPOConfig
import os
import csv
import logging
import warnings
import torch
import multiprocessing

# ==========================================
# 1. ç¯å¢ƒè®¾ç½®ä¸æ—¥å¿—æ¸…ç†
# ==========================================
os.environ["RAY_DEDUP_LOGS"] = "0"
warnings.filterwarnings("ignore")
logging.getLogger("ray").setLevel(logging.ERROR)

from env import DeliveryUAVEnv

def env_creator(env_config):
    max_steps = env_config.get("max_steps", 200)
    seed = env_config.get("seed", None)
    env = DeliveryUAVEnv(max_steps=max_steps, seed=seed)
    return ParallelPettingZooEnv(env)

def policy_mapping_fn(agent_id, episode, worker, **kwargs):
    if str(agent_id).startswith("rider_"):
        return "rider_policy"
    return "station_policy"

def main():
    # è‡ªåŠ¨æ£€æµ‹ CPUï¼Œä½†åœ¨è°ƒè¯•é˜¶æ®µæˆ‘ä»¬å…ˆæ‰‹åŠ¨é™åˆ¶ï¼Œé˜²æ­¢ WSL å†…å­˜ç‚¸äº†
    # å»ºè®®å…ˆè®¾ä¸º 4 ä¸ª Workerï¼Œç¨³å®šåå†æ…¢æ…¢å¾€ä¸ŠåŠ 
    default_workers = 4 

    parser = argparse.ArgumentParser()
    parser.add_argument("--iters", type=int, default=200)
    parser.add_argument("--max-steps", type=int, default=200)
    parser.add_argument("--num-workers", type=int, default=default_workers, help="CPUå¹¶è¡Œé‡‡æ ·è¿›ç¨‹æ•°")
    parser.add_argument("--num-envs-per-worker", type=int, default=5, help="æ¯ä¸ªè¿›ç¨‹å†…çš„å‘é‡åŒ–ç¯å¢ƒæ•°")
    parser.add_argument("--seed", type=int, default=0)
    args = parser.parse_args()

    # æ£€æŸ¥ GPU
    if torch.cuda.is_available():
        print(f"--- ğŸš€ æ˜¾å¡å·²å°±ç»ª: {torch.cuda.get_device_name(0)} ---")
        num_gpus = 1
    else:
        print("--- âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU ---")
        num_gpus = 0

    print(f"--- å¯åŠ¨é…ç½®: {args.num_workers} Workers x {args.num_envs_per_worker} Envs (æ€»å¹¶å‘: {args.num_workers * args.num_envs_per_worker}) ---")
    
    if ray.is_initialized():
        ray.shutdown()
    
    # å¢åŠ  object_store_memory é˜²æ­¢å†…å­˜æº¢å‡ºæŠ¥é”™ (å¯é€‰ï¼Œè§†æœºå™¨é…ç½®è€Œå®š)
    ray.init(ignore_reinit_error=True, log_to_driver=False)

    register_env("delivery_pz_env", env_creator)

    # --- è·å–ç©ºé—´ä¿¡æ¯ ---
    temp_env_instance = DeliveryUAVEnv(max_steps=args.max_steps, seed=0)
    rider_obs_space = temp_env_instance.observation_spaces["rider_0"]
    rider_act_space = temp_env_instance.action_spaces["rider_0"]
    station_obs_space = temp_env_instance.observation_spaces["station_0"]
    station_act_space = temp_env_instance.action_spaces["station_0"]
    
    policies = {
        "rider_policy": (None, rider_obs_space, rider_act_space, {}),
        "station_policy": (None, station_obs_space, station_act_space, {}),
    }

    # é‡æ–°è°ƒæ•´ Batch Size
    # 4 workers * 5 envs * 200 steps * 13 agents = 52,000
    # è®¾ä¸º 20000 ä¿è¯å¿«é€Ÿå“åº”
    train_batch_size = 20000 

    config = (
        PPOConfig()
        .api_stack(
            enable_rl_module_and_learner=False,
            enable_env_runner_and_connector_v2=False,
        )
        .environment(
            env="delivery_pz_env",
            env_config={"max_steps": args.max_steps, "seed": 0},
            disable_env_checking=True
        )
        .framework("torch")
        .env_runners(
            num_env_runners=args.num_workers,
            num_envs_per_env_runner=args.num_envs_per_worker,
            sample_timeout_s=600,
        )
        .resources(
            num_gpus=num_gpus 
        )
        .training(
            gamma=0.99,
            lr=3e-4,
            train_batch_size=train_batch_size, 
            minibatch_size=4096,
            # === [ä¿®æ­£] å‚æ•°æ”¹å: num_sgd_iter -> num_epochs ===
            num_epochs=5, 
            entropy_coeff=0.01,
            vf_loss_coeff=1.0,
        )
        .multi_agent(
            policies=policies,
            policy_mapping_fn=policy_mapping_fn,
            policies_to_train=["rider_policy", "station_policy"],
        )
    )

    algo = config.build_algo()

    log_filename = "training_wsl_gpu.csv"
    # åˆ›å»º checkpoints ç›®å½•ï¼Œé˜²æ­¢æŠ¥é”™
    os.makedirs("./checkpoints", exist_ok=True)
    print(f"--- è®­ç»ƒå¼€å§‹! æ—¥å¿—æ–‡ä»¶: {log_filename} ---")
    
    with open(log_filename, mode='w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Iteration", "Reward_Mean", "Episode_Len_Mean", "Total_Timesteps", "Steps_Per_Sec"])
        
        for i in range(args.iters):
            result = algo.train()
            
            metrics = result.get("env_runners", {}) or result
            mean_rew = metrics.get("episode_reward_mean", float('nan'))
            mean_len = metrics.get("episode_len_mean", 0)
            total_steps = result.get("num_env_steps_sampled", 0)
            fps = result.get("num_env_steps_sampled_throughput_per_sec", 0)

            print(f"Iter {i+1:03d}/{args.iters} | Reward: {mean_rew:.2f} | FPS: {fps:.0f} | TotalSteps: {total_steps}")
            writer.writerow([i+1, mean_rew, mean_len, total_steps, fps])
            f.flush()

            # [å…³é”®ä¿®æ”¹] ä¿å­˜æ¨¡å‹ Checkpoint
            if (i + 1) % 10 == 0: # æ¯10è½®å­˜ä¸€æ¬¡
                # æŒ‡å®šä¿å­˜è·¯å¾„ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
                save_dir = os.path.abspath(f"./checkpoints/iter_{i+1:04d}")
                checkpoint_path = algo.save(checkpoint_dir=save_dir)
                print(f"    --> æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}")

    print("--- è®­ç»ƒç»“æŸ ---")
    ray.shutdown()

if __name__ == "__main__":
    main()