import argparse
import os
import csv
import logging
import warnings
import torch
import ray

from ray.tune.registry import register_env
from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv
from ray.rllib.algorithms.appo import APPOConfig 
from env import DeliveryUAVEnv

# --- ç¯å¢ƒå˜é‡ ---
os.environ["OMP_NUM_THREADS"] = "1" 
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["RAY_DEDUP_LOGS"] = "0"
os.environ["RAY_USE_MULTIPROCESSING_CPU_COUNT"] = "1"

warnings.filterwarnings("ignore")
logging.getLogger("ray").setLevel(logging.ERROR)

def env_creator(env_config):
    max_steps = env_config.get("max_steps", 200)
    seed = env_config.get("seed", None)
    env = DeliveryUAVEnv(max_steps=max_steps, seed=seed)
    return ParallelPettingZooEnv(env)

# ä¿®æ”¹ Policy Mappingï¼Œå»æ‰ worker å‚æ•°ï¼Œæ”¹ç”¨ **kwargs
def policy_mapping_fn(agent_id, episode, **kwargs):
    if str(agent_id).startswith("rider_"):
        return "rider_policy"
    return "station_policy"



# ==========================================
# [æ–°å¢] è¯¾ç¨‹å­¦ä¹ è¾…åŠ©å‡½æ•°
# ==========================================
def get_current_prob(iteration):
    """
    è®¡ç®—å½“å‰å¼ºåˆ¶å»ç«™ç‚¹çš„æ¦‚ç‡ (Curriculum Schedule)
    ç­–ç•¥:
    - 0-100 è½®: 100% å¼ºåˆ¶ (è®© UAV ç–¯ç‹‚åˆ·æ•°æ®)
    - 100-250 è½®: çº¿æ€§è¡°å‡ (ä» 1.0 é™åˆ° 0.1)
    - 250+ è½®: ä¿æŒ 10% (ä¿ç•™ä¸€ç‚¹ç‚¹å¯å‘å¼å¼•å¯¼ï¼Œæˆ–è€…è®¾ä¸º0å®Œå…¨è‡ªä¸»)
    """
    length_period1 = 100
    length_period2 = 250
    min_pro = 0
    if iteration < length_period1:
        return 1.0
    elif iteration < length_period2:
        # çº¿æ€§æ’å€¼: éšç€ iter å¢åŠ ï¼Œprob å‡å°
        return max(min_pro, 1.0 - (iteration - length_period1) / (length_period2 - length_period1) * (1-min_pro))
    else:
        return min_pro 

def update_env_prob(env, context):
    """
    è¿™ä¸ªå‡½æ•°ä¼šè¢«å‘é€åˆ°æ¯ä¸ª Worker é‡Œæ‰§è¡Œ
    è´Ÿè´£æ‰¾åˆ°åº•å±‚çš„ DeliveryUAVEnv å¹¶ä¿®æ”¹æ¦‚ç‡
    """
    # å°è¯•ç©¿é€ ParallelPettingZooEnv åŒ…è£…å™¨æ‰¾åˆ°åŸå§‹ç¯å¢ƒ
    base_env = getattr(env, "par_env", None) or getattr(env, "unwrapped", None) or env
    
    # è°ƒç”¨ env.py ä¸­å®šä¹‰çš„æ¥å£
    if hasattr(base_env, "set_force_station_prob"):
        base_env.set_force_station_prob(context["prob"])



def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--iters", type=int, default=3000)
    parser.add_argument("--max-steps", type=int, default=200)
    parser.add_argument("--seed", type=int, default=0)
    args = parser.parse_args()

    # --- ç¡¬ä»¶é…ç½® ---
    total_cores = 25
    num_workers = 22 
    num_envs_per_worker = 5
    
    if torch.cuda.is_available():
        print(f"--- ğŸš€ æ˜¾å¡ç«åŠ›å…¨å¼€: {torch.cuda.get_device_name(0)} ---")
        num_gpus = 1
    else:
        num_gpus = 0

    print(f"--- ğŸ”¥ APPO æé€Ÿæ¨¡å¼ ---")
    print(f"    Workers: {num_workers} | Envs/Worker: {num_envs_per_worker}")
    
    if ray.is_initialized():
        ray.shutdown()
    
    ray.init(
        num_cpus=total_cores, 
        ignore_reinit_error=True, 
        log_to_driver=False, 
        include_dashboard=False
    )

    register_env("delivery_pz_env", env_creator)

    # è·å–ç©ºé—´
    temp_env = DeliveryUAVEnv(max_steps=args.max_steps, seed=0)
    rider_obs_space = temp_env.observation_spaces["rider_0"]
    rider_act_space = temp_env.action_spaces["rider_0"]
    station_obs_space = temp_env.observation_spaces["station_0"]
    station_act_space = temp_env.action_spaces["station_0"]
    temp_env.close()

    policies = {
        "rider_policy": (None, rider_obs_space, rider_act_space, {}),
        "station_policy": (None, station_obs_space, station_act_space, {}),
    }

    # --- APPO Config ---
    config = (
        APPOConfig()
        # ç¦ç”¨æ–° API Stack
        .api_stack(
            enable_rl_module_and_learner=False,
            enable_env_runner_and_connector_v2=False,
        )
        .environment(
            env="delivery_pz_env",
            env_config={"max_steps": args.max_steps, "seed": args.seed},
            disable_env_checking=True
        )
        .framework("torch")
        .env_runners(
            num_env_runners=num_workers,
            num_envs_per_env_runner=num_envs_per_worker,
            
            # ã€å…³é”®ä¿®æ”¹ã€‘APPO ä¸æ”¯æŒ 'auto'ï¼Œå¿…é¡»æ˜¯æ•´æ•°
            rollout_fragment_length=200, 
            
            num_cpus_per_env_runner=1,
        )
        .resources(
            num_gpus=num_gpus,
        )
        .training(
            # APPO æ¯æ¬¡æ›´æ–°çš„ Batch Size
            train_batch_size=8192, 
            entropy_coeff=0.001,
            lr=1e-4, 
            grad_clip=40.0,
            learner_queue_size=16,
            
            model={
                "fcnet_hiddens": [256, 256], 
                "fcnet_activation": "tanh",
            }
        )

        .multi_agent(
            policies=policies,
            policy_mapping_fn=policy_mapping_fn,
            policies_to_train=["rider_policy", "station_policy"],
        )
    )

    algo = config.build_algo()

    log_filename = "training_appo.csv"
    os.makedirs("./checkpoints", exist_ok=True)
    
    with open(log_filename, mode='w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Iteration", "Reward_Mean", "Episode_Len_Mean", "Total_Timesteps", "Steps_Per_Sec"])
        
        print(f"--- è®­ç»ƒå¼€å§‹! æ—¥å¿—: {log_filename} ---")
        
        for i in range(args.iters):
            # ================= [æ–°å¢] åŠ¨æ€è°ƒæ•´æ¦‚ç‡ =================
            # 1. è®¡ç®—å½“å‰è½®æ¬¡çš„æ¦‚ç‡
            current_prob = get_current_prob(i)
            
            # 2. å¹¿æ’­ç»™æ‰€æœ‰ Worker (å¹¶è¡Œç¯å¢ƒ)
            # æ–°ç‰ˆ Ray ä½¿ç”¨ env_runner_group æ›¿ä»£ workers
            algo.env_runner_group.foreach_env(
                lambda env: update_env_prob(env, {"prob": current_prob})
            )
            # =======================================================



            result = algo.train()
            
            metrics = result.get("env_runners", {}) or result
            mean_rew = metrics.get("episode_reward_mean", float('nan'))
            mean_len = metrics.get("episode_len_mean", 0)
            total_steps = result.get("num_env_steps_sampled", 0)
            fps = result.get("num_env_steps_sampled_throughput_per_sec", 0)
            timers = result.get("timers", {})


            print(f"Iter {i+1:03d} | FPS: {fps:.0f} | Rew: {mean_rew:.2f}")
            print(f"    ğŸ” Debug: {timers}")

            writer.writerow([i+1, mean_rew, mean_len, total_steps, fps])
            f.flush()

            if (i + 1) % 20 == 0: 
                save_dir = os.path.abspath(f"./checkpoints/iter_{i+1:04d}")
                algo.save(checkpoint_dir=save_dir)
                print(f"    --> æ¨¡å‹å·²ä¿å­˜")

    print("--- è®­ç»ƒç»“æŸ ---")
    ray.shutdown()

if __name__ == "__main__":
    main()