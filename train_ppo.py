import argparse
import os
import csv
import logging
import warnings
import multiprocessing
import torch

import ray
from ray.tune.registry import register_env
from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv
from ray.rllib.algorithms.ppo import PPOConfig

# å¼•å…¥ä½ çš„ç¯å¢ƒ
from env import DeliveryUAVEnv

# ==========================================
# 0. å…¨å±€é…ç½®ä¸æ¸…ç†
# ==========================================
os.environ["OMP_NUM_THREADS"] = "1" 
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["RAY_DEDUP_LOGS"] = "0"
# è®© Ray ç›¸ä¿¡æˆ‘ä»¬æ˜¯åœ¨å®¹å™¨é‡Œä¹Ÿè¦ç”¨å¤šæ ¸
os.environ["RAY_USE_MULTIPROCESSING_CPU_COUNT"] = "1" 

warnings.filterwarnings("ignore")
logging.getLogger("ray").setLevel(logging.ERROR)

# ==========================================
# 1. ç¯å¢ƒæ³¨å†Œå‡½æ•°
# ==========================================
def env_creator(env_config):
    max_steps = env_config.get("max_steps", 200)
    seed = env_config.get("seed", None)
    # ç¡®ä¿ä½ çš„ç¯å¢ƒç±»æ”¯æŒ seed å‚æ•°ï¼Œå¦‚æœä¸æ”¯æŒè¯·å»æ‰
    env = DeliveryUAVEnv(max_steps=max_steps, seed=seed)
    return ParallelPettingZooEnv(env)

def policy_mapping_fn(agent_id, episode, worker, **kwargs):
    if str(agent_id).startswith("rider_"):
        return "rider_policy"
    return "station_policy"

# ==========================================
# 2. ä¸»å‡½æ•°
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--iters", type=int, default=200)
    parser.add_argument("--max-steps", type=int, default=200)
    parser.add_argument("--seed", type=int, default=0)
    args = parser.parse_args()

    # --- ğŸ¤– ç¡¬ä»¶è‡ªåŠ¨æ£€æµ‹ (å…³é”®ä¼˜åŒ–) ---
    # è·å–ç‰©ç† CPU æ ¸å¿ƒæ•°
    total_cores = 25
    num_workers = max(1, total_cores - 2)
    
    # æ¯ä¸ª Worker å¹¶è¡Œè·‘çš„ç¯å¢ƒæ•°ã€‚
    # å¦‚æœç¯å¢ƒè®¡ç®—é‡å°ï¼Œå¯ä»¥è®¾å¤§ä¸€ç‚¹ (5-10)ã€‚å¦‚æœç¯å¢ƒå¾ˆé‡ï¼Œè®¾å°ä¸€ç‚¹ (1-2)ã€‚
    num_envs_per_worker = 5
    
    # è®¡ç®—æ€»å¹¶å‘æ•° (ç”¨äºæ£€æŸ¥)
    total_concurrency = num_workers * num_envs_per_worker

    # æ£€æŸ¥ GPU
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        num_gpus = 1
        print(f"--- ğŸš€ æ˜¾å¡ç«åŠ›å…¨å¼€: {gpu_name} ---")
    else:
        num_gpus = 0
        print("--- âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è®­ç»ƒ ---")

    print(f"--- ğŸ”¥ CPU æ€§èƒ½æ‹‰æ»¡æ¨¡å¼ ---")
    print(f"    æ£€æµ‹åˆ°æ€»æ ¸å¿ƒæ•°: {total_cores}")
    print(f"    åˆ†é… Workers: {num_workers}")
    print(f"    å• Worker ç¯å¢ƒæ•°: {num_envs_per_worker}")
    print(f"    æ€»å¹¶å‘é‡‡æ ·ç¯å¢ƒ: {total_concurrency}")
    print("-----------------------------")

    # --- Ray åˆå§‹åŒ– ---
    if ray.is_initialized():
        ray.shutdown()
    
    # æ˜¾å¼æŒ‡å®š num_cpusï¼Œå½»åº•è§£å†³ Docker æ£€æµ‹è­¦å‘Š
    ray.init(
        num_cpus=total_cores, 
        ignore_reinit_error=True, 
        log_to_driver=False, 
        include_dashboard=False
    )

    register_env("delivery_pz_env", env_creator)

    # --- è·å–ç©ºé—´ä¿¡æ¯ (Dummy Env) ---
    # åªéœ€è¦å®ä¾‹åŒ–ä¸€æ¬¡è·å– space å³å¯
    temp_env = DeliveryUAVEnv(max_steps=args.max_steps, seed=0)
    rider_obs_space = temp_env.observation_spaces["rider_0"]
    rider_act_space = temp_env.action_spaces["rider_0"]
    station_obs_space = temp_env.observation_spaces["station_0"]
    station_act_space = temp_env.action_spaces["station_0"]
    temp_env.close() # è®°å¾—å…³é—­

    policies = {
        "rider_policy": (None, rider_obs_space, rider_act_space, {}),
        "station_policy": (None, station_obs_space, station_act_space, {}),
    }

    # --- PPO å‚æ•°é…ç½® ---
    # å»ºè®®ï¼šBatch Size è®¾ä¸ºæ€»å¹¶å‘æ•°çš„æ•´æ•°å€ï¼Œæˆ–è€…è¶³å¤Ÿå¤§ä»¥è¦†ç›–å¤šæ¡è½¨è¿¹
    train_batch_size = 120000 
    sgd_minibatch_size = 8192 

    config = (
        PPOConfig()
        .api_stack(
            enable_rl_module_and_learner=False,
            enable_env_runner_and_connector_v2=False,
        )
        .environment(
            env="delivery_pz_env",
            env_config={"max_steps": args.max_steps, "seed": args.seed},
            disable_env_checking=True
        )
        .framework("torch")
        .env_runners(
            ####=========================####           
            batch_mode="truncate_episodes",
            rollout_fragment_length='auto',
            ####=========================####
            num_env_runners=num_workers,
            num_envs_per_env_runner=num_envs_per_worker,
            sample_timeout_s=600,
            num_cpus_per_env_runner=1,

        )
        .resources(
            num_gpus=num_gpus,
        )
        .training(
            gamma=0.99,
            lr=5e-4,
            train_batch_size=train_batch_size, 
            minibatch_size=sgd_minibatch_size, 
            num_epochs=5, 
            entropy_coeff=0.01,
            vf_loss_coeff=1.0,

            model={
                "fcnet_hiddens": [64, 64],  # åŸæ¥æ˜¯ [256, 256]
                "fcnet_activation": "tanh",
            }
        )
        .multi_agent(
            policies=policies,
            policy_mapping_fn=policy_mapping_fn,
            policies_to_train=["rider_policy", "station_policy"],
        )
    )

    algo = config.build_algo()

    # --- è®­ç»ƒå¾ªç¯ ---
    log_filename = "training_log_optimized.csv"
    os.makedirs("./checkpoints", exist_ok=True)
    
    with open(log_filename, mode='w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["Iteration", "Reward_Mean", "Episode_Len_Mean", "Total_Timesteps", "Steps_Per_Sec"])
        
        print(f"--- è®­ç»ƒå¼€å§‹! æ—¥å¿—: {log_filename} ---")
        
        for i in range(args.iters):
            result = algo.train()
            
            # æå–æŒ‡æ ‡
            metrics = result.get("env_runners", {}) or result
            mean_rew = metrics.get("episode_reward_mean", float('nan'))
            mean_len = metrics.get("episode_len_mean", 0)
            total_steps = result.get("num_env_steps_sampled", 0)
            # è¿™é‡Œçš„ throughput åŒ…å«äº† é‡‡æ ·+è®­ç»ƒ çš„ç»¼åˆé€Ÿåº¦
            fps = result.get("num_env_steps_sampled_throughput_per_sec", 0)

            timers = result.get("timers", {})
            sample_time = timers.get("env_runner_sampling_timer", 0) # é‡‡æ ·è€—æ—¶
            learn_time = timers.get("learner_grad_update_timer", 0)  # GPUè®­ç»ƒè€—æ—¶
            synch_time = timers.get("synch_weights_timer", 0)        # æƒé‡åŒæ­¥è€—æ—¶

            print(f"Iter {i+1:03d} | FPS: {fps:.0f} | Rew: {mean_rew:.2f}")
            print(f"    ğŸ” åŸå§‹è®¡æ—¶æ•°æ® (Debug): {timers}")

            #print(f"Iter {i+1:03d}/{args.iters} | Reward: {mean_rew:.2f} | FPS: {fps:.0f} | TotalSteps: {total_steps}")
            writer.writerow([i+1, mean_rew, mean_len, total_steps, fps])
            f.flush()

            if (i + 1) % 10 == 0: 
                save_dir = os.path.abspath(f"./checkpoints/iter_{i+1:04d}")
                algo.save(checkpoint_dir=save_dir)
                print(f"    --> æ¨¡å‹å·²ä¿å­˜")

    print("--- è®­ç»ƒç»“æŸ ---")
    ray.shutdown()

if __name__ == "__main__":
    main()